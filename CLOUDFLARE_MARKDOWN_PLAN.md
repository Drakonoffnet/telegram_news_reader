# Cloudflare Pages Markdown-Based Architecture Plan

## Overview

This plan transforms the Telegram News Reader into a **static content site** hosted on Cloudflare Pages, where:
- Backend runs as a scheduled job (cron) on any server/local machine
- Job fetches Telegram news and generates markdown files
- Markdown files + images are pushed to Cloudflare Pages
- Static site renders the markdown content dynamically

**Key Advantage:** No need to migrate complex backend - it stays wherever you want (local, VPS, etc.)

---

## Architecture Comparison

### Current Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     API      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend  ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   Backend    ‚îÇ
‚îÇ   (React)   ‚îÇ  fetch /news ‚îÇ  (FastAPI)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                             ‚îÇ   SQLite DB  ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                             ‚îÇ Telegram API ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### New Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Cloudflare Pages (Static Site)      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   HTML + JavaScript                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Markdown Renderer                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Navigation/Filtering             ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Static Files                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - news/*.md (markdown files)       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - images/*.jpg                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - index.json (metadata)            ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚ñ≤
                    ‚îÇ Git Push / Pages API
                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Backend Job (Runs Anywhere)               ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ  Cron Job / Scheduler                ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  1. Fetch from Telegram              ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  2. Generate Markdown Files          ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  3. Download Images                  ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  4. Create Index/Metadata            ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  5. Push to Git ‚Üí Cloudflare Pages   ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Implementation Plan

### Phase 1: Markdown Generator Backend Module

**File: `backend/markdown_generator.py`**

This module converts news from database into markdown files.

#### Markdown File Structure

Each news item becomes a markdown file:

```markdown
---
title: Channel Name - Message ID
channel: channel_name
channel_id: 1
group: Group Name
group_id: 1
date: 2025-11-29T10:30:00
message_id: 12345
image: /images/photo_2025_11_29_12345.jpg
---

# Channel Name

*Posted on November 29, 2025 at 10:30 AM*

This is the news content from Telegram message.
Multiple lines are preserved.

Links are clickable: https://example.com

![Image](/images/photo_2025_11_29_12345.jpg)
```

#### Directory Structure

```
output/
‚îú‚îÄ‚îÄ index.json                 # Metadata index for all news
‚îú‚îÄ‚îÄ channels.json             # List of channels and groups
‚îú‚îÄ‚îÄ news/
‚îÇ   ‚îú‚îÄ‚îÄ channel1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2025-11-29_msg12345.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2025-11-29_msg12346.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ channel2/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ latest.json           # Latest 50 news items metadata
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ photo_2025_11_29_12345.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ groups/
    ‚îú‚îÄ‚îÄ group1.json           # News IDs for group 1
    ‚îî‚îÄ‚îÄ group2.json           # News IDs for group 2
```

#### Code Implementation

```python
# backend/markdown_generator.py

import os
import json
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from sqlalchemy.orm import Session
from models import Channel, NewsItem, ChannelGroup
from database import SessionLocal


class MarkdownGenerator:
    def __init__(self, output_dir: str = "./output"):
        self.output_dir = Path(output_dir)
        self.news_dir = self.output_dir / "news"
        self.images_dir = self.output_dir / "images"
        self.groups_dir = self.output_dir / "groups"

    def clean_output(self):
        """Remove old output directory"""
        if self.output_dir.exists():
            shutil.rmtree(self.output_dir)

    def create_directories(self):
        """Create necessary directories"""
        self.news_dir.mkdir(parents=True, exist_ok=True)
        self.images_dir.mkdir(parents=True, exist_ok=True)
        self.groups_dir.mkdir(parents=True, exist_ok=True)

    def sanitize_filename(self, text: str) -> str:
        """Convert text to safe filename"""
        import re
        # Remove special characters, keep alphanumeric and basic punctuation
        safe = re.sub(r'[^\w\s-]', '', text)
        safe = re.sub(r'[-\s]+', '_', safe)
        return safe.lower()[:100]  # Limit length

    def generate_markdown(self, news_item: NewsItem, channel: Channel,
                         group: ChannelGroup = None) -> Dict:
        """Generate markdown file for a news item"""

        # Create channel directory
        channel_slug = self.sanitize_filename(channel.name)
        channel_dir = self.news_dir / channel_slug
        channel_dir.mkdir(exist_ok=True)

        # Generate filename
        date_str = news_item.date.strftime("%Y-%m-%d")
        filename = f"{date_str}_msg{news_item.message_id}.md"
        filepath = channel_dir / filename

        # Prepare frontmatter
        frontmatter = {
            "title": f"{channel.name} - {news_item.message_id}",
            "channel": channel.name,
            "channel_id": channel.id,
            "date": news_item.date.isoformat(),
            "message_id": news_item.message_id,
        }

        if group:
            frontmatter["group"] = group.name
            frontmatter["group_id"] = group.id

        if news_item.image_path:
            frontmatter["image"] = f"/images/{news_item.image_path}"

        # Build markdown content
        md_content = "---\n"
        for key, value in frontmatter.items():
            if isinstance(value, str):
                md_content += f'{key}: "{value}"\n'
            else:
                md_content += f'{key}: {value}\n'
        md_content += "---\n\n"

        # Add content
        md_content += f"# {channel.name}\n\n"
        md_content += f"*Posted on {news_item.date.strftime('%B %d, %Y at %I:%M %p')}*\n\n"

        if news_item.content:
            md_content += news_item.content + "\n\n"

        if news_item.image_path:
            md_content += f"![Image](/images/{news_item.image_path})\n"

        # Write file
        filepath.write_text(md_content, encoding='utf-8')

        # Return metadata
        return {
            "id": news_item.id,
            "file": f"news/{channel_slug}/{filename}",
            "channel": channel.name,
            "channel_slug": channel_slug,
            "group": group.name if group else None,
            "group_id": group.id if group else None,
            "date": news_item.date.isoformat(),
            "message_id": news_item.message_id,
            "image": f"/images/{news_item.image_path}" if news_item.image_path else None,
            "preview": news_item.content[:200] if news_item.content else ""
        }

    def copy_images(self, source_dir: str = "./backend/static/images"):
        """Copy images from source to output"""
        source = Path(source_dir)
        if source.exists():
            for img_file in source.iterdir():
                if img_file.is_file():
                    shutil.copy2(img_file, self.images_dir / img_file.name)

    def generate_all(self, db: Session, limit: int = 200):
        """Generate markdown files for all news items"""

        self.clean_output()
        self.create_directories()

        # Fetch news items
        news_items = db.query(NewsItem).order_by(
            NewsItem.date.desc()
        ).limit(limit).all()

        all_metadata = []
        channels_data = {}
        groups_data = {}

        for item in news_items:
            channel = item.channel
            group = channel.group if channel.group_id else None

            # Generate markdown
            metadata = self.generate_markdown(item, channel, group)
            all_metadata.append(metadata)

            # Organize by channel
            if channel.name not in channels_data:
                channels_data[channel.name] = {
                    "id": channel.id,
                    "name": channel.name,
                    "slug": self.sanitize_filename(channel.name),
                    "group": group.name if group else None,
                    "news": []
                }
            channels_data[channel.name]["news"].append(metadata["id"])

            # Organize by group
            if group:
                if group.name not in groups_data:
                    groups_data[group.name] = {
                        "id": group.id,
                        "name": group.name,
                        "news": []
                    }
                groups_data[group.name]["news"].append(metadata["id"])

        # Copy images
        self.copy_images()

        # Write index files
        (self.output_dir / "index.json").write_text(
            json.dumps(all_metadata, indent=2, ensure_ascii=False),
            encoding='utf-8'
        )

        (self.output_dir / "channels.json").write_text(
            json.dumps(list(channels_data.values()), indent=2, ensure_ascii=False),
            encoding='utf-8'
        )

        # Latest 50 for homepage
        (self.news_dir / "latest.json").write_text(
            json.dumps(all_metadata[:50], indent=2, ensure_ascii=False),
            encoding='utf-8'
        )

        # Group indexes
        for group_name, group_data in groups_data.items():
            group_file = self.groups_dir / f"{self.sanitize_filename(group_name)}.json"
            group_file.write_text(
                json.dumps(group_data, indent=2, ensure_ascii=False),
                encoding='utf-8'
            )

        print(f"Generated {len(all_metadata)} markdown files")
        print(f"Channels: {len(channels_data)}")
        print(f"Groups: {len(groups_data)}")
        print(f"Output directory: {self.output_dir.absolute()}")

        return all_metadata


def main():
    """Run markdown generation"""
    db = SessionLocal()
    try:
        generator = MarkdownGenerator()
        generator.generate_all(db, limit=200)
    finally:
        db.close()


if __name__ == "__main__":
    main()
```

---

### Phase 2: Cloudflare Pages Deployment Script

**File: `backend/deploy_to_cloudflare.py`**

This script pushes the generated files to Cloudflare Pages.

#### Option A: Git-Based Deployment (Recommended)

```python
# backend/deploy_to_cloudflare.py

import os
import subprocess
from pathlib import Path
from datetime import datetime


class CloudflareDeployer:
    def __init__(self,
                 output_dir: str = "./output",
                 repo_url: str = None,
                 branch: str = "main"):
        self.output_dir = Path(output_dir)
        self.repo_url = repo_url or os.getenv("CLOUDFLARE_PAGES_REPO")
        self.branch = branch
        self.git_dir = Path("./cloudflare-pages-repo")

    def clone_or_pull_repo(self):
        """Clone repo if not exists, otherwise pull latest"""
        if not self.git_dir.exists():
            print(f"Cloning repository: {self.repo_url}")
            subprocess.run([
                "git", "clone", self.repo_url, str(self.git_dir)
            ], check=True)
        else:
            print("Pulling latest changes...")
            subprocess.run([
                "git", "-C", str(self.git_dir),
                "pull", "origin", self.branch
            ], check=True)

    def copy_output_to_repo(self):
        """Copy generated files to git repository"""
        import shutil

        # Remove old content (except .git)
        for item in self.git_dir.iterdir():
            if item.name == ".git":
                continue
            if item.is_dir():
                shutil.rmtree(item)
            else:
                item.unlink()

        # Copy new content
        for item in self.output_dir.iterdir():
            dest = self.git_dir / item.name
            if item.is_dir():
                shutil.copytree(item, dest)
            else:
                shutil.copy2(item, dest)

        print(f"Copied {len(list(self.output_dir.iterdir()))} items to repository")

    def commit_and_push(self):
        """Commit changes and push to remote"""
        os.chdir(self.git_dir)

        # Check if there are changes
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            capture_output=True,
            text=True
        )

        if not result.stdout.strip():
            print("No changes to commit")
            return False

        # Add all changes
        subprocess.run(["git", "add", "."], check=True)

        # Commit
        commit_msg = f"Update news - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        subprocess.run([
            "git", "commit", "-m", commit_msg
        ], check=True)

        # Push
        subprocess.run([
            "git", "push", "origin", self.branch
        ], check=True)

        print(f"Pushed changes to {self.branch}")
        return True

    def deploy(self):
        """Full deployment process"""
        print("Starting Cloudflare Pages deployment...")
        self.clone_or_pull_repo()
        self.copy_output_to_repo()
        pushed = self.commit_and_push()

        if pushed:
            print("‚úÖ Deployment successful!")
            print("Cloudflare Pages will automatically build and deploy.")
        else:
            print("‚ÑπÔ∏è  No changes to deploy")


def main():
    """Run deployment"""
    deployer = CloudflareDeployer(
        repo_url=os.getenv("CLOUDFLARE_PAGES_REPO"),
        branch="main"
    )
    deployer.deploy()


if __name__ == "__main__":
    main()
```

#### Option B: Direct Cloudflare API Deployment

```python
# backend/deploy_to_cloudflare_api.py

import os
import requests
import zipfile
from pathlib import Path


class CloudflarePagesAPI:
    def __init__(self, account_id: str, project_name: str, api_token: str):
        self.account_id = account_id
        self.project_name = project_name
        self.api_token = api_token
        self.base_url = f"https://api.cloudflare.com/client/v4/accounts/{account_id}/pages/projects/{project_name}/deployments"
        self.headers = {
            "Authorization": f"Bearer {api_token}"
        }

    def create_zip(self, output_dir: str = "./output") -> str:
        """Create zip file of output directory"""
        zip_path = "./output.zip"
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file in Path(output_dir).rglob('*'):
                if file.is_file():
                    arcname = file.relative_to(output_dir)
                    zipf.write(file, arcname)

        print(f"Created zip: {zip_path}")
        return zip_path

    def deploy(self, output_dir: str = "./output"):
        """Deploy to Cloudflare Pages via API"""
        zip_path = self.create_zip(output_dir)

        with open(zip_path, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                self.base_url,
                headers=self.headers,
                files=files
            )

        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ Deployment successful!")
            print(f"Deployment URL: {result['result']['url']}")
        else:
            print(f"‚ùå Deployment failed: {response.text}")

        # Cleanup
        os.remove(zip_path)

        return response.status_code == 200


def main():
    """Run API deployment"""
    deployer = CloudflarePagesAPI(
        account_id=os.getenv("CLOUDFLARE_ACCOUNT_ID"),
        project_name=os.getenv("CLOUDFLARE_PROJECT_NAME"),
        api_token=os.getenv("CLOUDFLARE_API_TOKEN")
    )
    deployer.deploy()


if __name__ == "__main__":
    main()
```

---

### Phase 3: Static Site for Rendering Markdown

Create a simple HTML site that renders the markdown files.

**File Structure:**
```
static-site/
‚îú‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ news.html
‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îî‚îÄ‚îÄ style.css
‚îú‚îÄ‚îÄ js/
‚îÇ   ‚îú‚îÄ‚îÄ app.js
‚îÇ   ‚îî‚îÄ‚îÄ markdown-renderer.js
‚îî‚îÄ‚îÄ .nojekyll  # For GitHub Pages
```

**File: `static-site/index.html`**

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Telegram News Reader</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/style.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary sticky-top">
        <div class="container">
            <a class="navbar-brand" href="/">üì∞ Telegram News</a>
            <div class="ms-auto">
                <select id="groupFilter" class="form-select form-select-sm">
                    <option value="">All Groups</option>
                </select>
            </div>
        </div>
    </nav>

    <div class="container mt-4">
        <div id="loading" class="text-center py-5">
            <div class="spinner-border text-primary" role="status">
                <span class="visually-hidden">Loading...</span>
            </div>
        </div>

        <div id="newsContainer" class="row" style="display: none;">
            <!-- News items will be loaded here -->
        </div>

        <div id="noNews" class="text-center text-muted py-5" style="display: none;">
            No news available
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="/js/app.js"></script>
</body>
</html>
```

**File: `static-site/js/app.js`**

```javascript
// Load and render news
class NewsReader {
    constructor() {
        this.allNews = [];
        this.channels = [];
        this.groups = new Set();
        this.currentGroup = '';
    }

    async init() {
        await this.loadNews();
        this.setupFilters();
        this.renderNews();
    }

    async loadNews() {
        try {
            // Load latest news index
            const response = await fetch('/news/latest.json');
            this.allNews = await response.json();

            // Load channels
            const channelsResponse = await fetch('/channels.json');
            this.channels = await channelsResponse.json();

            // Extract unique groups
            this.allNews.forEach(item => {
                if (item.group) {
                    this.groups.add(item.group);
                }
            });

            console.log(`Loaded ${this.allNews.length} news items`);
        } catch (error) {
            console.error('Error loading news:', error);
        }
    }

    setupFilters() {
        const groupFilter = document.getElementById('groupFilter');

        // Populate group filter
        this.groups.forEach(group => {
            const option = document.createElement('option');
            option.value = group;
            option.textContent = group;
            groupFilter.appendChild(option);
        });

        // Handle filter change
        groupFilter.addEventListener('change', (e) => {
            this.currentGroup = e.target.value;
            this.renderNews();
        });
    }

    async renderNews() {
        const container = document.getElementById('newsContainer');
        const loading = document.getElementById('loading');
        const noNews = document.getElementById('noNews');

        loading.style.display = 'none';
        container.style.display = 'none';
        noNews.style.display = 'none';

        // Filter news
        let filteredNews = this.allNews;
        if (this.currentGroup) {
            filteredNews = this.allNews.filter(item => item.group === this.currentGroup);
        }

        if (filteredNews.length === 0) {
            noNews.style.display = 'block';
            return;
        }

        container.innerHTML = '';

        // Render each news item
        for (const newsItem of filteredNews) {
            const card = await this.createNewsCard(newsItem);
            container.appendChild(card);
        }

        container.style.display = 'block';
    }

    async createNewsCard(newsItem) {
        const col = document.createElement('div');
        col.className = 'col-12 mb-4';

        const card = document.createElement('div');
        card.className = 'card shadow-sm';

        // Card header
        const header = document.createElement('div');
        header.className = 'card-header d-flex justify-content-between align-items-center';
        header.innerHTML = `
            <strong>${newsItem.channel}</strong>
            <small class="text-muted">${new Date(newsItem.date).toLocaleString()}</small>
        `;

        // Card body
        const body = document.createElement('div');
        body.className = 'card-body';

        // Load and render markdown content
        try {
            const mdResponse = await fetch('/' + newsItem.file);
            const mdContent = await mdResponse.text();

            // Remove frontmatter
            const contentWithoutFrontmatter = mdContent.replace(/^---[\s\S]*?---\n/, '');

            // Render markdown
            body.innerHTML = marked.parse(contentWithoutFrontmatter);

            // Make images responsive
            body.querySelectorAll('img').forEach(img => {
                img.className = 'img-fluid rounded mb-3';
                img.style.maxHeight = '400px';
                img.style.objectFit = 'contain';
            });
        } catch (error) {
            console.error(`Error loading ${newsItem.file}:`, error);
            body.innerHTML = `<p class="text-danger">Error loading content</p>`;
        }

        card.appendChild(header);
        card.appendChild(body);
        col.appendChild(card);

        return col;
    }
}

// Initialize when DOM is ready
document.addEventListener('DOMContentLoaded', () => {
    const reader = new NewsReader();
    reader.init();
});
```

**File: `static-site/css/style.css`**

```css
body {
    background-color: #f8f9fa;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
}

.card {
    border: none;
    transition: transform 0.2s;
}

.card:hover {
    transform: translateY(-2px);
}

.card-header {
    background-color: #fff;
    border-bottom: 2px solid #f0f0f0;
}

pre {
    background-color: #f4f4f4;
    padding: 1rem;
    border-radius: 0.375rem;
    overflow-x: auto;
}

code {
    background-color: #f4f4f4;
    padding: 0.2rem 0.4rem;
    border-radius: 0.25rem;
    font-size: 0.875em;
}

pre code {
    background-color: transparent;
    padding: 0;
}
```

---

### Phase 4: Automated Job Script

**File: `backend/run_job.py`**

This combines everything into a single job that can run on a schedule.

```python
# backend/run_job.py

import os
import sys
from dotenv import load_dotenv
from datetime import datetime

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from telegram_service import fetch_all_channels
from markdown_generator import MarkdownGenerator
from deploy_to_cloudflare import CloudflareDeployer
from database import SessionLocal
import asyncio


async def run_job():
    """Main job function"""
    print(f"\n{'='*60}")
    print(f"Telegram News Job Started: {datetime.now()}")
    print(f"{'='*60}\n")

    try:
        # Step 1: Fetch latest news from Telegram
        print("üì° Step 1: Fetching news from Telegram...")
        await fetch_all_channels()
        print("‚úÖ News fetched successfully\n")

        # Step 2: Generate markdown files
        print("üìù Step 2: Generating markdown files...")
        db = SessionLocal()
        try:
            generator = MarkdownGenerator(output_dir="./output")
            metadata = generator.generate_all(db, limit=200)
            print(f"‚úÖ Generated {len(metadata)} markdown files\n")
        finally:
            db.close()

        # Step 3: Deploy to Cloudflare Pages
        print("üöÄ Step 3: Deploying to Cloudflare Pages...")
        deployer = CloudflareDeployer(
            output_dir="./output",
            repo_url=os.getenv("CLOUDFLARE_PAGES_REPO"),
            branch="main"
        )
        deployer.deploy()
        print("‚úÖ Deployment complete\n")

        print(f"{'='*60}")
        print(f"‚úÖ Job Completed Successfully: {datetime.now()}")
        print(f"{'='*60}\n")

    except Exception as e:
        print(f"\n‚ùå Job Failed: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    load_dotenv()
    asyncio.run(run_job())
```

---

### Phase 5: Scheduling the Job

#### Option A: Linux Cron (Recommended for VPS/Server)

```bash
# Edit crontab
crontab -e

# Add this line to run every hour
0 * * * * cd /path/to/telegram_news_reader/backend && /usr/bin/python3 run_job.py >> /var/log/telegram_news_job.log 2>&1
```

#### Option B: systemd Timer (Linux)

**File: `/etc/systemd/system/telegram-news.service`**

```ini
[Unit]
Description=Telegram News Fetch and Deploy
After=network.target

[Service]
Type=oneshot
User=your_user
WorkingDirectory=/path/to/telegram_news_reader/backend
Environment="PATH=/usr/bin:/usr/local/bin"
ExecStart=/usr/bin/python3 run_job.py
StandardOutput=journal
StandardError=journal
```

**File: `/etc/systemd/system/telegram-news.timer`**

```ini
[Unit]
Description=Run Telegram News Job Hourly
Requires=telegram-news.service

[Timer]
OnCalendar=hourly
Persistent=true

[Install]
WantedBy=timers.target
```

**Enable:**
```bash
sudo systemctl enable telegram-news.timer
sudo systemctl start telegram-news.timer
```

#### Option C: Python APScheduler (Current System)

Update `backend/scheduler.py`:

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from run_job import run_job

scheduler = AsyncIOScheduler()

def start_scheduler():
    # Run every hour
    scheduler.add_job(run_job, 'interval', hours=1)
    # Run immediately on startup
    scheduler.add_job(run_job)
    scheduler.start()
```

#### Option D: GitHub Actions (Run Anywhere)

**File: `.github/workflows/fetch-news.yml`**

```yaml
name: Fetch and Deploy Telegram News

on:
  schedule:
    - cron: '0 * * * *'  # Every hour
  workflow_dispatch:  # Manual trigger

jobs:
  fetch-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: Run job
        env:
          TELEGRAM_API_ID: ${{ secrets.TELEGRAM_API_ID }}
          TELEGRAM_API_HASH: ${{ secrets.TELEGRAM_API_HASH }}
          CLOUDFLARE_PAGES_REPO: ${{ secrets.CLOUDFLARE_PAGES_REPO }}
        run: |
          cd backend
          python run_job.py
```

---

## Setup Instructions

### Step 1: Prepare Backend

1. **Add new dependencies to `requirements.txt`:**
```txt
# Existing dependencies...
GitPython==3.1.40
```

2. **Install dependencies:**
```bash
cd backend
pip install -r requirements.txt
```

3. **Create the new modules:**
   - `markdown_generator.py`
   - `deploy_to_cloudflare.py`
   - `run_job.py`

### Step 2: Create Cloudflare Pages Project

**Option A: Git-based (Recommended)**

1. Create a new GitHub repository: `telegram-news-static`

2. Clone it locally:
```bash
git clone https://github.com/yourusername/telegram-news-static.git cloudflare-pages-repo
cd cloudflare-pages-repo
```

3. Add the static site files:
```bash
# Copy static-site/* files to the repo
cp -r ../static-site/* .
git add .
git commit -m "Initial static site"
git push origin main
```

4. Create Cloudflare Pages project:
   - Go to Cloudflare Dashboard ‚Üí Pages
   - Click "Create a project"
   - Connect to GitHub
   - Select `telegram-news-static` repository
   - Build settings:
     - Framework: None
     - Build command: (leave empty)
     - Build output directory: `/`
   - Deploy

**Option B: Direct Upload**

1. Create Pages project via Cloudflare Dashboard
2. Use Wrangler CLI or API for deployment

### Step 3: Configure Environment Variables

Create `.env` file in backend:

```bash
# Telegram
TELEGRAM_API_ID=your_api_id
TELEGRAM_API_HASH=your_api_hash

# Cloudflare Pages (Git method)
CLOUDFLARE_PAGES_REPO=https://github.com/yourusername/telegram-news-static.git

# Or Cloudflare Pages (API method)
CLOUDFLARE_ACCOUNT_ID=your_account_id
CLOUDFLARE_PROJECT_NAME=telegram-news
CLOUDFLARE_API_TOKEN=your_api_token
```

### Step 4: Test the Job

Run manually to test:

```bash
cd backend
python run_job.py
```

Expected output:
```
=================================================
Telegram News Job Started: 2025-11-29 10:30:00
=================================================

üì° Step 1: Fetching news from Telegram...
‚úÖ News fetched successfully

üìù Step 2: Generating markdown files...
Generated 150 markdown files
Channels: 5
Groups: 2
‚úÖ Generated 150 markdown files

üöÄ Step 3: Deploying to Cloudflare Pages...
Copied 4 items to repository
Pushed changes to main
‚úÖ Deployment complete

=================================================
‚úÖ Job Completed Successfully: 2025-11-29 10:35:00
=================================================
```

### Step 5: Setup Automation

Choose one of the scheduling options from Phase 5.

For cron (simplest):
```bash
crontab -e
# Add:
0 * * * * cd /path/to/telegram_news_reader/backend && python3 run_job.py >> /tmp/telegram_news_job.log 2>&1
```

---

## Advantages of This Approach

### ‚úÖ Pros

1. **No Backend Migration Needed**
   - Keep Python/Telethon code as-is
   - Run anywhere (local machine, VPS, Raspberry Pi, GitHub Actions)

2. **Cloudflare Benefits**
   - Global CDN
   - Free hosting (Pages free tier)
   - DDoS protection
   - Automatic HTTPS
   - Fast performance

3. **Offline-First Content**
   - All content is static files
   - No database queries at runtime
   - Works even if backend is down

4. **SEO Friendly**
   - Static markdown ‚Üí HTML is crawlable
   - Fast page load times
   - Pre-rendered content

5. **Simple Architecture**
   - No complex API calls
   - No authentication needed
   - Easy to debug

6. **Version Control**
   - All content in Git
   - Full history of changes
   - Easy rollback

7. **Flexible Hosting**
   - Can also deploy to GitHub Pages, Netlify, Vercel
   - Not locked into Cloudflare

### ‚ö†Ô∏è Limitations

1. **Not Real-Time**
   - Content updates only when job runs (e.g., hourly)
   - Can't show live updates

2. **File Count Limits**
   - Many static files (one per news item)
   - Might hit filesystem limits with 10,000+ items
   - Mitigation: Limit to latest 200-500 items

3. **No User Interactivity**
   - Can't add channels via web UI
   - Must manage channels on backend

4. **Build Time**
   - Generating and pushing files takes time
   - Not instant like API

---

## Cost Analysis

### Cloudflare Pages Free Tier
- ‚úÖ Unlimited requests
- ‚úÖ Unlimited bandwidth
- ‚úÖ 500 builds/month (more than enough for hourly = 720/month)
- ‚úÖ 20k files max (we'll have ~200-500)

### Backend Hosting
- **Option 1:** Run on local machine - **$0**
- **Option 2:** Raspberry Pi - **$35 one-time**
- **Option 3:** Small VPS (DigitalOcean, Linode) - **$5/month**
- **Option 4:** GitHub Actions (free tier) - **$0**

### Total Cost
**$0 - $5/month** (vs. $20-50/month for full cloud hosting)

---

## Migration Timeline

| Phase | Task | Time |
|-------|------|------|
| 1 | Create markdown_generator.py | 2-3 hours |
| 2 | Create deploy_to_cloudflare.py | 1-2 hours |
| 3 | Create static site (HTML/JS/CSS) | 3-4 hours |
| 4 | Create run_job.py | 30 min |
| 5 | Setup Cloudflare Pages | 30 min |
| 6 | Testing & debugging | 2-3 hours |
| 7 | Setup scheduling | 30 min |

**Total: 1-2 days**

---

## Next Steps

1. ‚úÖ Review this plan
2. Create `markdown_generator.py` module
3. Create static site files
4. Test markdown generation locally
5. Create Cloudflare Pages project
6. Test deployment
7. Setup automation
8. Monitor and optimize

---

## Questions to Answer

1. **How many news items to keep?**
   - Recommend: Latest 200-500 items
   - Older items archived or deleted

2. **Update frequency?**
   - Recommend: Every 1-2 hours
   - Can go as frequent as every 15 minutes

3. **Where to run the job?**
   - Local machine (during development)
   - VPS (for production)
   - GitHub Actions (serverless option)

4. **Git or API deployment?**
   - Git (recommended): Simpler, version controlled
   - API: Faster, no git repo needed

Would you like me to proceed with implementing any of these components?
